---
title: "modeling_rf_xgb_nnet_stack_log"
author: "Junxiong Liu"
date: "July 29, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse=TRUE, prompt=TRUE, eval=TRUE, message=F, include=T,comment=NULL,fig.width = 5, warning = FALSE, fig.height = 3,tidy.opts=list(width.cutoff=50),tidy=TRUE,cache = TRUE)
```

```{r packageCheck, include=FALSE}
mypacks <- c("tidyverse","car","tidyr","cvTools","rpart","nnet","gbm","yaImpute","randomForest", "xgboost","readr")  # what packages are needed?
packs <- installed.packages()   # find installed package list
install.me <- mypacks[!(mypacks %in% packs[,"Package"])]  #what needs to be installed?
if (length(install.me) >= 1) install.packages(install.me, repos = "http://cran.us.r-project.org")   # install (if needed)
lapply(mypacks, library, character.only=TRUE)  # load all packages
```

# Suggestion: Write functions to make this fully reproducible/easy-to-run.

# "horizontal" stacking of different models

# ppl in kernel add "log_leak" column into training. We don't.. -- we are at rmse roughly 1.20 at training set

#### may currently focus on using whole train

# Read in help functions
```{r}
source("../help_functions/preparation&cleaning_functions.R")
source("../help_functions/modeling_functions.R")
```

# logged response

#-------------------------------------------------------
# Now start operations:

## Read in train data
```{r}
Sys.time()

# just use only non-leaky portion to train and predict non-leaky test
# train <- read.csv("../../../data/cleaned/leaky_combined_0730/train_noleak.csv")

# still use the whole "train" to train.
train <- read.csv("../../../data/raw/train.csv")

train_noid <- train %>% 
  select(-ID) %>%
  mutate(target = log(target))
Sys.time()
```

#-------------------------------------------------------
## General Cleaning: Data Quality/Unsupervised method (non-response related)

### General Cleaning (gc) step 1: get rid of invalid features (min=max)
```{r}
Sys.time()
train_gc_s1 <- noVariation_filter(train_noid)
Sys.time()
```

### General Cleaning (gc) step 2: get rid of highly correlated features (disabled)
```{r}
# Sys.time()
# temp <- train_gc_s1 %>% select(-target)
# threshold <- 0.95 # get rid of > 0.95 correlations
# decorr_feas <- collinear_filter(temp, threshold)
# to_select <- c(decorr_feas, "target")
# train_gc_s2 <- train_gc_s1 %>% select(to_select)
# Sys.time()
```

#-------------------------------------------------------
## Response-related and fit

### general initialization (careful about too many features at this step) -- can result in slow training
```{r}
num_top <- 50 # number of top features
# num_top <- 100 # number of top features
# num_top <- 200
# num_top <- 300 # updated
# num_top <- 30
train_1 <- train_gc_s1
```

### feature selection with random forest and feature engineering
```{r}
Sys.time()
# Initialize
num_top_1 <- num_top
cur_response_1 <- "target"
train_1_small_1 <- rf_selection(train_1, cur_response_1, num_top_1)
rf_features_1 <- base::setdiff(names(train_1_small_1), c(cur_response_1)) # for testing correspondence

# row-wise feature engineering
train_1_small_2 <- rw_fea_engineering(train_1_small_1, cur_response_1)
# get final features
init_features_1 <- base::setdiff(names(train_1_small_2), c(cur_response_1))
Sys.time()
```

### General Level 1 -- stacking

#### fit xgboost
```{r}
# xgboost fit
dtrain_r1 <- xgb.DMatrix(as.matrix(train_1_small_2[, init_features_1]), 
                         label=train_1_small_2[,cur_response_1])
xgb_best_r1 <- my_xgb(dtrain_r1)
dfit_r1 <- xgb.DMatrix(as.matrix(train_1_small_2[, init_features_1])) # fit on train for prediction
Sys.time()
```

#### fit random forest
```{r}
rf_best_r1 <- my_rf(train_1_small_2, cur_response_1)
# current r2 at this stage
# mean(rf_best_r1$rsq)
Sys.time()
```

#### ensemble the models results using a xgboost model
```{r}
# get predictions
train_1_small_3 <- train_1_small_2 %>% 
  mutate(r1_pred_xgb = predict(xgb_best_r1, dfit_r1),
         r1_pred_rf = predict(rf_best_r1))

# get ensemble features and train a xgb
ensemble_features_1 <- c("r1_pred_xgb", "r1_pred_rf") #,...)
dtrain_ens_r1 <- xgb.DMatrix(as.matrix(train_1_small_3[, ensemble_features_1]), 
                         label=train_1_small_3[,cur_response_1]) # still same target
xgb_best_ens_r1 <- my_xgb(dtrain_ens_r1, depth = 100000, lr = 0.001, es = 5) 

# will need a super deep tree to accomodate small learning rate (i.e. 0.001) -- currently: 0.82 test rmse with lr=0.03 -- able to converge within 2500 for ~60 features

# for next level/checking "overall" training rmse
dfit_ens_r1 <- xgb.DMatrix(as.matrix(train_1_small_3[, ensemble_features_1])) # fit on train for prediction
Sys.time()
```


#### quickly check the current "overall" training rmse to get an idea
```{r}
train_check <- train_1_small_3 %>%
  mutate(final_predicted = predict(xgb_best_ens_r1, dfit_ens_r1)) %>%
  select(final_predicted, target)

eval(train_check, "final_predicted", "target")
```


##--------------------------------------------------------------------------------------------

## Read in the test set and predict
```{r}
test <- read_csv("../../../data/cleaned/leaky_combined_0730/test_noleak.csv")
test_leaky <- read_csv("../../../data/cleaned/leaky_combined_0730/test_leak.csv")
```

## let the test column match the train column
### note: if the test column starts with a number, will add an x to let it match the train columns
```{r}
all_test_col <- names(test)
all_test_col_new <- c() # new ones

for (each in all_test_col){
  each_new <- each
  
  if (is.na(as.numeric(substring(each, 1, 1))) == FALSE){ 
    # this col starts with a number
    each_new <- paste("X", each, sep = "")
  }
  all_test_col_new <- c(all_test_col_new, each_new)
  colnames(test)[which(names(test) == each)] <- each_new
}
```

## now predict stuff
```{r}
## for xgboost in training initial round
test_r1 <- rw_fea_engineering(test[,rf_features_1]) # generate the row-wise features for test
dtest_r1 <- xgb.DMatrix(as.matrix(test_r1[, init_features_1]))

## individual results round 1
test_r1_ind_res <- test %>% 
  mutate(r1_pred_xgb = predict(xgb_best_r1, dtest_r1),
         r1_pred_rf = predict(rf_best_r1, newdata = test_r1))

## ensemble results round 1
dtest_r1_ens <- xgb.DMatrix(as.matrix(test_r1_ind_res[, ensemble_features_1]))
test_r1_ens <- test_r1_ind_res %>%
  mutate(target = predict(xgb_best_ens_r1, dtest_r1_ens)) %>% 
  select(ID, 
         r1_pred_xgb,
         r1_pred_rf,
         #....,
         target) # for view

## exponential to get final results
out <- test_r1_ens %>% 
  mutate(target = exp(target)) %>% # get the exponential
  select(-starts_with("r1"))
```

## combine with test_leaky
```{r}
test_leaky_out <- test_leaky %>% 
  select(ID, compiled_leak) %>%
  rename(target = compiled_leak)

final_out <- bind_rows(out, test_leaky_out)
```

## write out
```{r}
write.csv(final_out, "../../../data/predictions/lk_predictions/lk_trainwhole_top50rf_rfxgbens_fullTuned.csv", 
          row.names = FALSE)
```


```{r}

```


















